{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unita: GDT\n",
    "\n",
    "Semplice classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Partiamo dall'output folder dell'unit√† 04_notMNIST specificando dove e' posizionato il file pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = \"DeepLearning/data/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\nValidation set (10000, 28, 28) (10000,)\nTest set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  \n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.02156863  0.17058824  0.1627451   0.16666667\n   0.17450981  0.18235295  0.18627451  0.19411765  0.19803922  0.20588236\n   0.20980392  0.21372549  0.22156863  0.22941177  0.22941177  0.2372549\n   0.24117647  0.25686276 -0.00196078]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5         0.28431374  0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.49607843  0.5\n   0.21764706]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5         0.32745099  0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.49607843  0.48823529  0.48823529\n   0.48823529  0.48823529  0.49607843  0.5         0.5         0.5         0.5\n   0.30784315]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.14705883 -0.04901961 -0.04901961 -0.0372549\n  -0.02941176 -0.02156863 -0.00588235 -0.03333334  0.39411765  0.5         0.5\n   0.5         0.49607843  0.5         0.19019608 -0.0254902   0.02941176\n   0.03333334 -0.07254902]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.28823531  0.5         0.49607843\n   0.49607843  0.49607843  0.5        -0.12352941 -0.5        -0.48039216\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.48823529 -0.48431373 -0.48431373 -0.48431373\n  -0.48431373 -0.48431373 -0.46862745 -0.5         0.31176472  0.5\n   0.49607843  0.49607843  0.49215686  0.5        -0.0882353  -0.5\n  -0.46470588 -0.48431373 -0.48823529]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.31960785  0.5         0.49607843\n   0.49607843  0.48823529  0.5        -0.08431373 -0.5        -0.48039216\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.33137256  0.5         0.49607843\n   0.5         0.48431373  0.5        -0.06862745 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.34313726  0.5         0.49607843\n   0.5         0.48039216  0.5        -0.05294118 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.35882354  0.5         0.49607843\n   0.5         0.48039216  0.5        -0.04117647 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.3509804   0.5         0.49607843\n   0.5         0.47647059  0.5        -0.02156863 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.28823531  0.5         0.49607843\n   0.5         0.47647059  0.5        -0.01764706 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.22156863  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.15490197  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.09215686  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.0254902   0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.49215686 -0.5        -0.04117647  0.49607843  0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.49607843 -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373\n  -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49215686 -0.5        -0.10784314  0.48823529\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.49215686 -0.5        -0.17450981  0.48039216  0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.40196079 -0.11960784 -0.11960784 -0.10784314 -0.09607843 -0.09607843\n  -0.0372549  -0.4137255  -0.5        -0.49607843 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.24117647  0.47254902\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.24901961  0.5         0.49215686  0.5         0.5         0.47647059\n   0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.30784315  0.46862745\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.30000001  0.5         0.48039216  0.48823529  0.48823529  0.46470588\n   0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.37058824  0.46078432\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.33137256  0.5         0.49215686  0.5         0.5         0.47647059\n   0.5        -0.30000001 -0.5        -0.47647059 -0.48431373 -0.48431373\n  -0.48431373 -0.48431373 -0.48431373 -0.48039216 -0.49215686 -0.46862745\n   0.44901961  0.5         0.5         0.47647059  0.5        -0.02156863\n  -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.37058824  0.5         0.48039216  0.5         0.5         0.47647059\n   0.5        -0.28039217 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.37058824  0.46078432\n   0.5         0.49607843  0.46470588  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.41764706  0.44901961  0.5         0.49215686  0.49215686  0.49607843\n   0.5         0.40588236  0.18627451  0.00980392  0.00588235  0.00980392\n   0.01372549  0.02156863  0.02941176  0.02156863  0.20196079  0.45294118\n   0.5         0.49607843  0.48431373  0.5         0.5        -0.13137256\n  -0.5        -0.48039216 -0.5        -0.5       ]\n [-0.5        -0.37058824  0.15882353  0.49607843  0.49607843  0.49215686\n   0.48431373  0.49607843  0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.49607843  0.48823529\n   0.5         0.5         0.28431374 -0.28039217 -0.5        -0.49215686\n  -0.5        -0.5        -0.5       ]\n [-0.5        -0.5        -0.49607843 -0.24117647  0.28823531  0.5\n   0.49607843  0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.4254902  -0.07647059 -0.48039216 -0.49607843 -0.49607843 -0.5        -0.5\n  -0.5        -0.5       ]\n [-0.5        -0.49215686 -0.48431373 -0.5        -0.46078432 -0.04509804\n   0.2254902   0.20588236  0.20588236  0.19803922  0.19411765  0.19019608\n   0.18627451  0.18235295  0.17450981  0.16666667  0.1627451   0.16666667\n   0.1        -0.3509804  -0.5        -0.49215686 -0.48823529 -0.5        -0.5\n  -0.5        -0.5        -0.5       ]]\n()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\nValidation set (10000, 784) (10000, 10)\nTest set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.02156863  0.17058824  0.1627451   0.16666667\n  0.17450981  0.18235295  0.18627451  0.19411765  0.19803922  0.20588236\n  0.20980392  0.21372549  0.22156863  0.22941177  0.22941177  0.2372549\n  0.24117647  0.25686276 -0.00196078 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5         0.28431374\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.5         0.21764706 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  0.32745099  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.48823529  0.48823529  0.48823529  0.48823529\n  0.49607843  0.5         0.5         0.5         0.5         0.30784315\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.14705883 -0.04901961 -0.04901961 -0.0372549\n -0.02941176 -0.02156863 -0.00588235 -0.03333334  0.39411765  0.5         0.5\n  0.5         0.49607843  0.5         0.19019608 -0.0254902   0.02941176\n  0.03333334 -0.07254902 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.28823531\n  0.5         0.49607843  0.49607843  0.49607843  0.5        -0.12352941\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n -0.46862745 -0.5         0.31176472  0.5         0.49607843  0.49607843\n  0.49215686  0.5        -0.0882353  -0.5        -0.46470588 -0.48431373\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.48431373 -0.5         0.31960785  0.5\n  0.49607843  0.49607843  0.48823529  0.5        -0.08431373 -0.5\n -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.48431373 -0.5\n  0.33137256  0.5         0.49607843  0.5         0.48431373  0.5\n -0.06862745 -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.34313726  0.5         0.49607843  0.5\n  0.48039216  0.5        -0.05294118 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48431373 -0.5         0.35882354  0.5         0.49607843\n  0.5         0.48039216  0.5        -0.04117647 -0.5        -0.47647059\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.3509804\n  0.5         0.49607843  0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.48431373\n -0.5         0.28823531  0.5         0.49607843  0.5         0.47647059\n  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.22156863  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.15490197  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.09215686  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.0254902   0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49215686 -0.5        -0.04117647  0.49607843  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.49607843 -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373\n -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49215686 -0.5        -0.10784314  0.48823529\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.49215686 -0.5\n -0.17450981  0.48039216  0.5         0.5         0.47647059  0.5\n -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.40196079\n -0.11960784 -0.11960784 -0.10784314 -0.09607843 -0.09607843 -0.0372549\n -0.4137255  -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49607843 -0.5        -0.24117647  0.47254902  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.24901961  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.5        -0.30784315  0.46862745\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.30000001  0.5         0.48039216\n  0.48823529  0.48823529  0.46470588  0.5        -0.30000001 -0.5\n -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.33137256  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.47647059 -0.48431373 -0.48431373\n -0.48431373 -0.48431373 -0.48431373 -0.48039216 -0.49215686 -0.46862745\n  0.44901961  0.5         0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.37058824  0.5\n  0.48039216  0.5         0.5         0.47647059  0.5        -0.28039217\n -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.49607843\n  0.46470588  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.41764706  0.44901961  0.5         0.49215686  0.49215686  0.49607843\n  0.5         0.40588236  0.18627451  0.00980392  0.00588235  0.00980392\n  0.01372549  0.02156863  0.02941176  0.02156863  0.20196079  0.45294118\n  0.5         0.49607843  0.48431373  0.5         0.5        -0.13137256\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.37058824\n  0.15882353  0.49607843  0.49607843  0.49215686  0.48431373  0.49607843\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.49607843  0.48823529  0.5         0.5\n  0.28431374 -0.28039217 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.24117647  0.28823531  0.5\n  0.49607843  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5\n  0.4254902  -0.07647059 -0.48039216 -0.49607843 -0.49607843 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.49215686 -0.48431373 -0.5\n -0.46078432 -0.04509804  0.2254902   0.20588236  0.20588236  0.19803922\n  0.19411765  0.19019608  0.18627451  0.18235295  0.17450981  0.16666667\n  0.1627451   0.16666667  0.1        -0.3509804  -0.5        -0.49215686\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5       ]\n(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #1\n",
    "\n",
    "Implementare una logistic regression multinomiale con discesa del gradiente con Tensorflow (TF) \n",
    "come classificatore per notMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei pensi W viene spesso inizializzata con una variabile casuale con distribuzione normale,\n",
    "dove i valori maggiori di 2 x std_dev sono rimossi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZLUYThIkjr+PyYzxDq6QS/SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116798490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights = tf.Variable( tf.truncated_normal(...))\n",
    "\n",
    "# Questo permette di ignorare valori troppo grandi o piccoli che possono influenzare negativamente l'apprendimento.\n",
    "\n",
    "n = 500000\n",
    "A = tf.truncated_normal((n,))\n",
    "B = tf.random_normal((n,))\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([A, B])\n",
    "\n",
    "pl.hist(a, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.00000000e+00,   4.00000000e+00,   7.00000000e+00,\n          8.00000000e+00,   1.40000000e+01,   1.30000000e+01,\n          2.50000000e+01,   3.00000000e+01,   3.30000000e+01,\n          4.80000000e+01,   6.70000000e+01,   9.10000000e+01,\n          1.29000000e+02,   1.79000000e+02,   1.89000000e+02,\n          2.37000000e+02,   3.70000000e+02,   4.34000000e+02,\n          5.12000000e+02,   5.54000000e+02,   7.85000000e+02,\n          9.41000000e+02,   1.15400000e+03,   1.38800000e+03,\n          1.74400000e+03,   2.00300000e+03,   2.38500000e+03,\n          2.83200000e+03,   3.31200000e+03,   3.74300000e+03,\n          4.34800000e+03,   5.12700000e+03,   5.59000000e+03,\n          6.53600000e+03,   7.15500000e+03,   8.06000000e+03,\n          8.70400000e+03,   9.72100000e+03,   1.05110000e+04,\n          1.12840000e+04,   1.22190000e+04,   1.31190000e+04,\n          1.37970000e+04,   1.45460000e+04,   1.49800000e+04,\n          1.55710000e+04,   1.60720000e+04,   1.63220000e+04,\n          1.67650000e+04,   1.67900000e+04,   1.66690000e+04,\n          1.66020000e+04,   1.64060000e+04,   1.60490000e+04,\n          1.56120000e+04,   1.51810000e+04,   1.41680000e+04,\n          1.35550000e+04,   1.29950000e+04,   1.21560000e+04,\n          1.12160000e+04,   1.05330000e+04,   9.62900000e+03,\n          8.74800000e+03,   8.03400000e+03,   7.23000000e+03,\n          6.41600000e+03,   5.64800000e+03,   5.13300000e+03,\n          4.45900000e+03,   3.79200000e+03,   3.25300000e+03,\n          2.78400000e+03,   2.32200000e+03,   1.99500000e+03,\n          1.67100000e+03,   1.40200000e+03,   1.17000000e+03,\n          9.66000000e+02,   7.59000000e+02,   6.17000000e+02,\n          5.08000000e+02,   4.39000000e+02,   3.32000000e+02,\n          2.89000000e+02,   1.80000000e+02,   1.47000000e+02,\n          1.27000000e+02,   8.10000000e+01,   6.30000000e+01,\n          6.00000000e+01,   3.70000000e+01,   3.00000000e+01,\n          2.10000000e+01,   1.20000000e+01,   1.40000000e+01,\n          1.20000000e+01,   4.00000000e+00,   8.00000000e+00,\n          5.00000000e+00]),\n array([-4.2  , -4.116, -4.032, -3.948, -3.864, -3.78 , -3.696, -3.612,\n        -3.528, -3.444, -3.36 , -3.276, -3.192, -3.108, -3.024, -2.94 ,\n        -2.856, -2.772, -2.688, -2.604, -2.52 , -2.436, -2.352, -2.268,\n        -2.184, -2.1  , -2.016, -1.932, -1.848, -1.764, -1.68 , -1.596,\n        -1.512, -1.428, -1.344, -1.26 , -1.176, -1.092, -1.008, -0.924,\n        -0.84 , -0.756, -0.672, -0.588, -0.504, -0.42 , -0.336, -0.252,\n        -0.168, -0.084,  0.   ,  0.084,  0.168,  0.252,  0.336,  0.42 ,\n         0.504,  0.588,  0.672,  0.756,  0.84 ,  0.924,  1.008,  1.092,\n         1.176,  1.26 ,  1.344,  1.428,  1.512,  1.596,  1.68 ,  1.764,\n         1.848,  1.932,  2.016,  2.1  ,  2.184,  2.268,  2.352,  2.436,\n         2.52 ,  2.604,  2.688,  2.772,  2.856,  2.94 ,  3.024,  3.108,\n         3.192,  3.276,  3.36 ,  3.444,  3.528,  3.612,  3.696,  3.78 ,\n         3.864,  3.948,  4.032,  4.116,  4.2  ]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11961cad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.hist(b, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "\n",
    "train_subset = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su Tensorflow ogni elemento - input, variabili ed elaborazioni - √® descritto mediante un grafo, o dataflow graph. Gli oggetti tf.Operation rappresentano unit√† di computazione;\n",
    "\n",
    "Gli oggetti tf.Tensor rappresentano unit√† di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "\n",
    "¬∑ La struttura: nodi e archi che rappresentano le operazioni \n",
    "\n",
    "¬∑ Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si pu√≤ ispezionare con tf.get_collection.\n",
    "\n",
    "TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "Un oggetto Graph di default √® sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, che sostituisce il grafo di default per tutta l'esistenza del contesto in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  # Il vettore di bias b e' inizializzato a 0.\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rate = 0.5 (alfa nelle slide di richiami sulle reti neurali.)\n",
    "  # La funzione minimize e' composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'e' backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo gia' fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  \n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "                     \n",
    "  test_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco l'accuratezza come somma del numero di predizioni corrette normalizzato sul numero di predizioni totali.\n",
    "\n",
    "La uso per fare statistiche durante il funzionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nLoss at step 0: 18.513605\nTraining accuracy: 10.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 12.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 100: 2.347272\nTraining accuracy: 71.2%\nValidation accuracy: 70.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 200: 1.861626\nTraining accuracy: 74.2%\nValidation accuracy: 73.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 300: 1.608356\nTraining accuracy: 75.6%\nValidation accuracy: 74.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400: 1.441480\nTraining accuracy: 76.4%\nValidation accuracy: 74.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500: 1.319018\nTraining accuracy: 77.1%\nValidation accuracy: 75.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 600: 1.223978\nTraining accuracy: 77.7%\nValidation accuracy: 75.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 700: 1.147300\nTraining accuracy: 78.1%\nValidation accuracy: 75.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 800: 1.083506\nTraining accuracy: 78.5%\nValidation accuracy: 75.2%\nTest accuracy: 10.3%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poiche' la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cioe' lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  \n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cioe': weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    \n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cioe' un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    \n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      \n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #2\n",
    " \n",
    "Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "\n",
    "Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "\n",
    "L'elaborazione √® ripetuta, perci√≤ conviene scrivere il codice senza gestire la creazione dei dati direttamente.\n",
    "\n",
    "In TF un placeholder √® una variabile che assumera i valori a tempo di esecuzione.\n",
    "\n",
    "Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori durante l'elaborazione.\n",
    "\n",
    "#### x = tf.placeholder(\"float\", None)\n",
    "#### y = x * 2\n",
    "#### print(x) \n",
    "\n",
    "#### Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "\n",
    "Stampa solo il tipo e non il valore di x.\n",
    "In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture piu' complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Una immagine a colori (RGB) in formato raw pu√≤ avere una rappresentazione matriciale\n",
    "\n",
    "#### image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #3 (da completare dopo la lezione \"2017 ML-07-MLP_RELU\")\n",
    "\n",
    "Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "N.B. la funzione tf.nn.relu() restituisce un tensore che calcola la RELU sul tensore di input.\n",
    "L'output ha la stessa dimensione dell'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
