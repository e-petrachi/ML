{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unità: GDT\n",
    "\n",
    "### Classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Partiamo dall'output folder dell'unità 04_notMNIST specificando dove e' posizionato il file pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = \"DeepLearning/data/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\nValidation set (10000, 28, 28) (10000,)\nTest set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  \n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\nValidation set (10000, 784) (10000, 10)\nTest set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.02156863  0.17058824  0.1627451   0.16666667\n  0.17450981  0.18235295  0.18627451  0.19411765  0.19803922  0.20588236\n  0.20980392  0.21372549  0.22156863  0.22941177  0.22941177  0.2372549\n  0.24117647  0.25686276 -0.00196078 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5         0.28431374\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.5         0.21764706 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  0.32745099  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.48823529  0.48823529  0.48823529  0.48823529\n  0.49607843  0.5         0.5         0.5         0.5         0.30784315\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.14705883 -0.04901961 -0.04901961 -0.0372549\n -0.02941176 -0.02156863 -0.00588235 -0.03333334  0.39411765  0.5         0.5\n  0.5         0.49607843  0.5         0.19019608 -0.0254902   0.02941176\n  0.03333334 -0.07254902 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.28823531\n  0.5         0.49607843  0.49607843  0.49607843  0.5        -0.12352941\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n -0.46862745 -0.5         0.31176472  0.5         0.49607843  0.49607843\n  0.49215686  0.5        -0.0882353  -0.5        -0.46470588 -0.48431373\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.48431373 -0.5         0.31960785  0.5\n  0.49607843  0.49607843  0.48823529  0.5        -0.08431373 -0.5\n -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.48431373 -0.5\n  0.33137256  0.5         0.49607843  0.5         0.48431373  0.5\n -0.06862745 -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.34313726  0.5         0.49607843  0.5\n  0.48039216  0.5        -0.05294118 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48431373 -0.5         0.35882354  0.5         0.49607843\n  0.5         0.48039216  0.5        -0.04117647 -0.5        -0.47647059\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.3509804\n  0.5         0.49607843  0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.48431373\n -0.5         0.28823531  0.5         0.49607843  0.5         0.47647059\n  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.22156863  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.15490197  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.09215686  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.0254902   0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49215686 -0.5        -0.04117647  0.49607843  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.49607843 -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373\n -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49215686 -0.5        -0.10784314  0.48823529\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.49215686 -0.5\n -0.17450981  0.48039216  0.5         0.5         0.47647059  0.5\n -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.40196079\n -0.11960784 -0.11960784 -0.10784314 -0.09607843 -0.09607843 -0.0372549\n -0.4137255  -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49607843 -0.5        -0.24117647  0.47254902  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.24901961  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.5        -0.30784315  0.46862745\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.30000001  0.5         0.48039216\n  0.48823529  0.48823529  0.46470588  0.5        -0.30000001 -0.5\n -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.33137256  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.47647059 -0.48431373 -0.48431373\n -0.48431373 -0.48431373 -0.48431373 -0.48039216 -0.49215686 -0.46862745\n  0.44901961  0.5         0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.37058824  0.5\n  0.48039216  0.5         0.5         0.47647059  0.5        -0.28039217\n -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.49607843\n  0.46470588  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.41764706  0.44901961  0.5         0.49215686  0.49215686  0.49607843\n  0.5         0.40588236  0.18627451  0.00980392  0.00588235  0.00980392\n  0.01372549  0.02156863  0.02941176  0.02156863  0.20196079  0.45294118\n  0.5         0.49607843  0.48431373  0.5         0.5        -0.13137256\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.37058824\n  0.15882353  0.49607843  0.49607843  0.49215686  0.48431373  0.49607843\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.49607843  0.48823529  0.5         0.5\n  0.28431374 -0.28039217 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.24117647  0.28823531  0.5\n  0.49607843  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5\n  0.4254902  -0.07647059 -0.48039216 -0.49607843 -0.49607843 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.49215686 -0.48431373 -0.5\n -0.46078432 -0.04509804  0.2254902   0.20588236  0.20588236  0.19803922\n  0.19411765  0.19019608  0.18627451  0.18235295  0.17450981  0.16666667\n  0.1627451   0.16666667  0.1        -0.3509804  -0.5        -0.49215686\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5       ]\n(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei pensi W viene spesso inizializzata con una variabile casuale con distribuzione normale,\n",
    "dove i valori maggiori di 2 x std_dev sono rimossi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,   1968.,   2977.,\n          3424.,   4052.,   4473.,   5184.,   5921.,   6697.,   7571.,\n          8398.,   9212.,  10076.,  10956.,  11802.,  12745.,  13892.,\n         14602.,  14954.,  15882.,  16480.,  16863.,  17055.,  17468.,\n         17488.,  17610.,  17447.,  17054.,  16868.,  16230.,  15764.,\n         15450.,  14325.,  13440.,  12835.,  11977.,  10947.,   9919.,\n          9342.,   8401.,   7502.,   6679.,   5916.,   5299.,   4646.,\n          3933.,   3310.,   2932.,   2034.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.]),\n array([-4.2  , -4.116, -4.032, -3.948, -3.864, -3.78 , -3.696, -3.612,\n        -3.528, -3.444, -3.36 , -3.276, -3.192, -3.108, -3.024, -2.94 ,\n        -2.856, -2.772, -2.688, -2.604, -2.52 , -2.436, -2.352, -2.268,\n        -2.184, -2.1  , -2.016, -1.932, -1.848, -1.764, -1.68 , -1.596,\n        -1.512, -1.428, -1.344, -1.26 , -1.176, -1.092, -1.008, -0.924,\n        -0.84 , -0.756, -0.672, -0.588, -0.504, -0.42 , -0.336, -0.252,\n        -0.168, -0.084,  0.   ,  0.084,  0.168,  0.252,  0.336,  0.42 ,\n         0.504,  0.588,  0.672,  0.756,  0.84 ,  0.924,  1.008,  1.092,\n         1.176,  1.26 ,  1.344,  1.428,  1.512,  1.596,  1.68 ,  1.764,\n         1.848,  1.932,  2.016,  2.1  ,  2.184,  2.268,  2.352,  2.436,\n         2.52 ,  2.604,  2.688,  2.772,  2.856,  2.94 ,  3.024,  3.108,\n         3.192,  3.276,  3.36 ,  3.444,  3.528,  3.612,  3.696,  3.78 ,\n         3.864,  3.948,  4.032,  4.116,  4.2  ]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6jAcJEkdhoMkqcNwkCR1/H+unLc3IyN1kAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11961b110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights = tf.Variable( tf.truncated_normal(...))\n",
    "\n",
    "# Questo permette di ignorare valori troppo grandi o piccoli che possono influenzare negativamente l'apprendimento.\n",
    "\n",
    "n = 500000\n",
    "A = tf.truncated_normal((n,))\n",
    "B = tf.random_normal((n,))\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([A, B])\n",
    "\n",
    "pl.hist(a, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.00000000e+00,   3.00000000e+00,   4.00000000e+00,\n          1.00000000e+01,   1.30000000e+01,   1.80000000e+01,\n          2.40000000e+01,   2.30000000e+01,   4.30000000e+01,\n          4.90000000e+01,   7.20000000e+01,   9.50000000e+01,\n          1.20000000e+02,   1.45000000e+02,   1.90000000e+02,\n          2.81000000e+02,   2.95000000e+02,   3.99000000e+02,\n          5.29000000e+02,   6.18000000e+02,   7.96000000e+02,\n          1.00200000e+03,   1.20600000e+03,   1.33400000e+03,\n          1.72000000e+03,   1.99700000e+03,   2.42600000e+03,\n          2.79200000e+03,   3.29200000e+03,   3.89100000e+03,\n          4.43600000e+03,   5.03500000e+03,   5.58200000e+03,\n          6.38300000e+03,   7.16700000e+03,   7.92200000e+03,\n          8.75700000e+03,   9.64200000e+03,   1.06340000e+04,\n          1.13960000e+04,   1.23150000e+04,   1.29150000e+04,\n          1.38630000e+04,   1.43570000e+04,   1.51340000e+04,\n          1.55520000e+04,   1.59970000e+04,   1.63530000e+04,\n          1.68150000e+04,   1.65610000e+04,   1.66990000e+04,\n          1.67040000e+04,   1.61330000e+04,   1.60160000e+04,\n          1.56280000e+04,   1.51730000e+04,   1.46090000e+04,\n          1.36700000e+04,   1.29840000e+04,   1.22500000e+04,\n          1.15410000e+04,   1.04810000e+04,   9.52600000e+03,\n          8.66400000e+03,   8.12300000e+03,   7.28800000e+03,\n          6.45700000e+03,   5.57500000e+03,   4.85200000e+03,\n          4.46900000e+03,   3.72900000e+03,   3.26000000e+03,\n          2.78000000e+03,   2.41200000e+03,   1.97500000e+03,\n          1.66000000e+03,   1.31500000e+03,   1.21600000e+03,\n          9.21000000e+02,   7.75000000e+02,   6.23000000e+02,\n          5.00000000e+02,   3.75000000e+02,   3.33000000e+02,\n          2.56000000e+02,   2.12000000e+02,   1.69000000e+02,\n          1.17000000e+02,   7.50000000e+01,   5.60000000e+01,\n          5.30000000e+01,   3.60000000e+01,   3.40000000e+01,\n          1.90000000e+01,   8.00000000e+00,   1.00000000e+01,\n          8.00000000e+00,   4.00000000e+00,   5.00000000e+00,\n          5.00000000e+00]),\n array([-4.2  , -4.116, -4.032, -3.948, -3.864, -3.78 , -3.696, -3.612,\n        -3.528, -3.444, -3.36 , -3.276, -3.192, -3.108, -3.024, -2.94 ,\n        -2.856, -2.772, -2.688, -2.604, -2.52 , -2.436, -2.352, -2.268,\n        -2.184, -2.1  , -2.016, -1.932, -1.848, -1.764, -1.68 , -1.596,\n        -1.512, -1.428, -1.344, -1.26 , -1.176, -1.092, -1.008, -0.924,\n        -0.84 , -0.756, -0.672, -0.588, -0.504, -0.42 , -0.336, -0.252,\n        -0.168, -0.084,  0.   ,  0.084,  0.168,  0.252,  0.336,  0.42 ,\n         0.504,  0.588,  0.672,  0.756,  0.84 ,  0.924,  1.008,  1.092,\n         1.176,  1.26 ,  1.344,  1.428,  1.512,  1.596,  1.68 ,  1.764,\n         1.848,  1.932,  2.016,  2.1  ,  2.184,  2.268,  2.352,  2.436,\n         2.52 ,  2.604,  2.688,  2.772,  2.856,  2.94 ,  3.024,  3.108,\n         3.192,  3.276,  3.36 ,  3.444,  3.528,  3.612,  3.696,  3.78 ,\n         3.864,  3.948,  4.032,  4.116,  4.2  ]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1195a8b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.hist(b, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "\n",
    "train_subset = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su Tensorflow ogni elemento - input, variabili ed elaborazioni - è descritto mediante un grafo, o dataflow graph. Gli oggetti tf.Operation rappresentano unità di computazione;\n",
    "\n",
    "Gli oggetti tf.Tensor rappresentano unità di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "\n",
    "· La struttura: nodi e archi che rappresentano le operazioni \n",
    "\n",
    "· Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si può ispezionare con tf.get_collection.\n",
    "\n",
    "TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "Un oggetto Graph di default è sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, che sostituisce il grafo di default per tutta l'esistenza del contesto in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  # Il vettore di bias b e' inizializzato a 0.\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rate = 0.5 (alfa nelle slide di richiami sulle reti neurali.)\n",
    "  # La funzione minimize e' composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'e' backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo gia' fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  \n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "                     \n",
    "  test_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco l'accuratezza come somma del numero di predizioni corrette normalizzato sul numero di predizioni totali.\n",
    "\n",
    "La uso per fare statistiche durante il funzionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 16.168175\nTraining accuracy: 15.6%\nValidation accuracy: 18.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 100: 2.316981\nTraining accuracy: 71.1%\nValidation accuracy: 70.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 200: 1.869234\nTraining accuracy: 74.5%\nValidation accuracy: 72.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 300: 1.626184\nTraining accuracy: 76.0%\nValidation accuracy: 73.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400: 1.458901\nTraining accuracy: 76.8%\nValidation accuracy: 74.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500: 1.333366\nTraining accuracy: 77.3%\nValidation accuracy: 74.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 600: 1.234621\nTraining accuracy: 77.8%\nValidation accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 700: 1.154623\nTraining accuracy: 78.2%\nValidation accuracy: 75.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 800: 1.088382\nTraining accuracy: 78.9%\nValidation accuracy: 75.4%\nTest accuracy: 9.9%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poiche' la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cioe' lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  \n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cioe': weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    \n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cioe' un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    \n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      \n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #2\n",
    " \n",
    "Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "\n",
    "Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "\n",
    "L'elaborazione è ripetuta, perciò conviene scrivere il codice senza gestire la creazione dei dati direttamente.\n",
    "\n",
    "In TF un placeholder è una variabile che assumera i valori a tempo di esecuzione.\n",
    "\n",
    "Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori durante l'elaborazione.\n",
    "\n",
    "#### x = tf.placeholder(\"float\", None)\n",
    "#### y = x * 2\n",
    "#### print(x) \n",
    "\n",
    "#### Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "\n",
    "Stampa solo il tipo e non il valore di x.\n",
    "In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture piu' complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Una immagine a colori (RGB) in formato raw può avere una rappresentazione matriciale\n",
    "\n",
    "#### image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Una operazione placeholder viene usata per alimentare il grafo.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "  # Il resto e' uguale al precedente esempio.\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nMinibatch loss at step 0: 18.011261\nMinibatch accuracy: 10.2%\nValidation accuracy: 13.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 1.778531\nMinibatch accuracy: 73.4%\nValidation accuracy: 75.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 1.439653\nMinibatch accuracy: 73.4%\nValidation accuracy: 76.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1500: 1.192041\nMinibatch accuracy: 78.9%\nValidation accuracy: 76.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 1.092158\nMinibatch accuracy: 78.9%\nValidation accuracy: 78.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2500: 1.123341\nMinibatch accuracy: 74.2%\nValidation accuracy: 78.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 1.058553\nMinibatch accuracy: 75.8%\nValidation accuracy: 78.8%\nTest accuracy: 86.4%\n"
     ]
    }
   ],
   "source": [
    "# Se impiego minibatch potenzialmente ho piu' varianza nell'apprendimento ad ogni ciclo.\n",
    "# Sono costretto ad aumentare gli step.\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Definisco un offset nel trainig set\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Estraggo ilminibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    # Dizionario {chiave_placeholder : valore, ...}\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #3\n",
    "\n",
    "Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "N.B. la funzione tf.nn.relu() restituisce un tensore che calcola la RELU sul tensore di input.\n",
    "\n",
    "L'output ha la stessa dimensione dell'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodi del hidden layer\n",
    "hidden_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Avendo un layer in piu' ho due coppie <W,B>\n",
    "    # N.B. W1 e B1 hanno dimensioni <#input-feature-vector,#hidden-nodes>, <#hidden-nodes>\n",
    "    # mentre W2 <#hidden-nodes,num_labels>, <num_labels>\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "\n",
    "    # Ora la loss function e' definita sullo layer di output\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits \\\n",
    "                          (labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    # Seguo la stessa pipeline per valid e test set\n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    valid_prediction = tf.nn.softmax(logits_2) \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    test_prediction = tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nMinibatch loss at step 0: 304.169861\nMinibatch accuracy: 7.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 34.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 11.532445\nMinibatch accuracy: 77.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 20.276548\nMinibatch accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1500: 9.447702\nMinibatch accuracy: 78.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 10.407885\nMinibatch accuracy: 83.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2500: 5.001930\nMinibatch accuracy: 83.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 3.171875\nMinibatch accuracy: 85.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta molto più rapidamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
