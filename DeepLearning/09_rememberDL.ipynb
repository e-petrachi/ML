{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Python 2 and 3 Compatibility Library\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = \"DeepLearning/data/notMNIST.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  \n",
    "  train_data = save['train_dataset']\n",
    "  train_target = save['train_labels']\n",
    "  \n",
    "  valid_data = save['valid_dataset']\n",
    "  valid_target = save['valid_labels']\n",
    "  \n",
    "  test_data = save['test_dataset']\n",
    "  test_target = save['test_labels']\n",
    "  \n",
    "  del save  \n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size_pool(input_size, conv_filter_size, pool_filter_size, padding, conv_stride, pool_stride):\n",
    "    if padding == 'SAME':\n",
    "        padding = -1.00\n",
    "    elif padding == 'VALID':\n",
    "        padding = 0.00\n",
    "    else:\n",
    "        return None\n",
    "    # Dopo la prima convoluzione\n",
    "    output_1 = (((input_size - conv_filter_size - 2*padding) / conv_stride) + 1.00)\n",
    "    # Dopo il primo pooling\n",
    "    # Uso la stessa formula poiche' il pooling ha lo stride pari a 2, \n",
    "    # percio' riduce l'input come la convoluzione\n",
    "    output_2 = (((output_1 - pool_filter_size - 2*padding) / pool_stride) + 1.00)    \n",
    "    # Dopo la seconda convoluzione\n",
    "    output_3 = (((output_2 - conv_filter_size - 2*padding) / conv_stride) + 1.00)\n",
    "    # Dopo il secondo pooling\n",
    "    output_4 = (((output_3 - pool_filter_size - 2*padding) / pool_stride) + 1.00)  \n",
    "    return int(output_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPERPARAMETRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# INPUT\n",
    "#\n",
    "# La dimensione delle immagini 28x28\n",
    "image_size = 28\n",
    "# la dimensione della colonna target 10x1\n",
    "num_labels = 10\n",
    "# La profondita' dei dati in input\n",
    "num_channels = 1 \n",
    "# 1 = scala di grigi\n",
    "\n",
    "#\n",
    "# DATASET \n",
    "#\n",
    "train_data, train_target = reformat(train_data, train_target)\n",
    "valid_data, valid_target = reformat(valid_data, valid_target)\n",
    "test_data,   test_target = reformat(test_data,   test_target)\n",
    "\n",
    "#\n",
    "# FILTERS\n",
    "#\n",
    "# Dimensione del minibatch\n",
    "batch_size = 16\n",
    "# Dimensione della finestra per la convoluzione 5x5\n",
    "patch_size = 5\n",
    "# Numero di features (o profondita') del filtro\n",
    "depth = 16\n",
    "# Nodi dell'hidden layer\n",
    "num_hidden = 64\n",
    "# Numero di celle che il filtro scorre ad ogni iterazione\n",
    "stride = 1\n",
    "# Dimensione della finestra per il pooling 2x2\n",
    "pool_size = 2 \n",
    "# Numero di celle che il pooling scorre ad ogni iterazione\n",
    "pool_stride = 2\n",
    "# Probabilita' del Dropout\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITETTURA TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 4 for 'MatMul' (op: 'MatMul') with input shapes: [200000,256], [4,4,16,64].",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b708475dd759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# ESECUZIONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n\u001b[1;32m    110\u001b[0m                                     labels=tf_train_target, logits=logits))  \n",
      "\u001b[0;32m<ipython-input-16-b708475dd759>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# RELU LAYER3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         hidden_3 = tf.nn.relu(tf.matmul(reshape, \n\u001b[0;32m---> 95\u001b[0;31m                                       tf_layer3_pesi) + tf_layer3_bias)\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# DROPOUT LAYER3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mhidden_drop_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1801\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \"\"\"\n\u001b[1;32m   1262\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1263\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1264\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Administrator/anaconda/envs/gl-env/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 4 for 'MatMul' (op: 'MatMul') with input shapes: [200000,256], [4,4,16,64]."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Per i minibatch utilizzo **placeholder**\n",
    "    tf_train_data = tf.placeholder(tf.float32,\n",
    "                              shape=(\n",
    "                                  batch_size, image_size, num_channels))\n",
    "    tf_train_target = tf.placeholder(tf.float32,\n",
    "                              shape=(\n",
    "                                  batch_size,             num_channels))\n",
    "    \n",
    "    # Senza minibatch\n",
    "    tf_valid_data = tf.constant(valid_data)\n",
    "    tf_test_data  = tf.constant(test_data)\n",
    "    \n",
    "    # Layer1 CNN\n",
    "    # Dimens   = l_finestra, h_finestra, prof_input, prof_output\n",
    "    dim_layer1 = [patch_size, patch_size, num_channels, depth]\n",
    "    # Pesi RANDOM + TRUNCATE valori troppo bassi o alti\n",
    "    tf_layer1_pesi = tf.Variable(tf.truncated_normal(dim_layer1, stddev=0.1))\n",
    "    # Bias RANDOM (0)\n",
    "    tf_layer1_bias = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    # Layer2 CNN\n",
    "    # Dimens   = l_finestra, h_finestra, prof_input, prof_output\n",
    "    dim_layer2 = [patch_size, patch_size, depth, depth]\n",
    "    # Pesi RANDOM + TRUNCATE \n",
    "    tf_layer2_pesi = tf.Variable(tf.truncated_normal(dim_layer2, stddev=0.1))\n",
    "    # Bias RANDOM (1)\n",
    "    tf_layer2_bias = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    # Layer3 FULL\n",
    "    # Ricalcolo la dimensione del layer in base alla nuova configurazione \n",
    "    ##final_image_size = output_size_pool(image_size, patch_size, pool_size, \n",
    "    ##                                    'VALID', stride, pool_stride)\n",
    "    # Dimens   = l_input(dopo2CNN), h_input(dopo2CNN), prof_input, prof_output\n",
    "    dim_layer3 = [image_size/4, image_size/4, depth, num_hidden]\n",
    "    # Pesi RANDOM + TRUNCATE \n",
    "    tf_layer3_pesi = tf.Variable(tf.truncated_normal(dim_layer3, stddev=0.1))\n",
    "    # Bias RANDOM (1)\n",
    "    tf_layer3_bias = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    # Layer4 FULL\n",
    "    # Dimens   = lunghezza, altezza\n",
    "    dim_layer4 = [num_hidden, num_hidden]\n",
    "    # Pesi RANDOM + TRUNCATE \n",
    "    tf_layer4_pesi = tf.Variable(tf.truncated_normal(dim_layer4, stddev=0.1))\n",
    "    # Bias RANDOM (1)\n",
    "    tf_layer4_bias = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    # Layer5 FULL\n",
    "    # Dimens   = lunghezza, altezza\n",
    "    dim_layer5 = [num_hidden, num_labels]\n",
    "    # Pesi RANDOM + TRUNCATE \n",
    "    tf_layer5_pesi = tf.Variable(tf.truncated_normal(dim_layer5, stddev=0.1))\n",
    "    # Bias RANDOM (1)\n",
    "    tf_layer5_bias = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # MODELLO\n",
    "    # \n",
    "    def model(data):\n",
    "        # Convolutional LAYER1\n",
    "        conv_1 = tf.nn.conv2d(data, tf_layer1_pesi, \n",
    "                              strides=[1, stride, stride, 1], \n",
    "                              padding='VALID')\n",
    "        # RELU LAYER1\n",
    "        hidden_1 = tf.nn.relu(conv_1 + tf_layer1_bias)\n",
    "        # Average POOLING LAYER1\n",
    "        pool_1 = tf.nn.avg_pool(hidden_1, \n",
    "                                [1, pool_size, pool_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], \n",
    "                                padding='VALID')\n",
    "        \n",
    "        # Convolutional LAYER2\n",
    "        conv_2 = tf.nn.conv2d(pool_1, tf_layer2_pesi, \n",
    "                              strides=[1, stride, stride, 1], \n",
    "                              padding='VALID')\n",
    "        # RELU LAYER2\n",
    "        hidden_2 = tf.nn.relu(conv_2 + tf_layer2_bias)\n",
    "        # Average POOLING LAYER2\n",
    "        pool_2 = tf.nn.avg_pool(hidden_2, \n",
    "                                [1, pool_size, pool_size, 1], \n",
    "                                [1, pool_stride, pool_stride, 1], \n",
    "                                padding='VALID')\n",
    "        \n",
    "        # Full Connected LAYER3\n",
    "        shape = pool_2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool_2, \n",
    "                            [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        # RELU LAYER3\n",
    "        hidden_3 = tf.nn.relu(tf.matmul(reshape, \n",
    "                                      tf_layer3_pesi) + tf_layer3_bias)\n",
    "        # DROPOUT LAYER3\n",
    "        hidden_drop_3 = tf.nn.dropout(hidden_3, keep_prob)\n",
    "        \n",
    "        # Full Connected RELU LAYER4\n",
    "        hidden_4 = tf.nn.relu(tf.matmul(hidden_drop_3, \n",
    "                                        tf_layer4_pesi) + tf_layer4_bias)\n",
    "        # DROPOUT LAYER4\n",
    "        hidden_drop_4 = tf.nn.dropout(hidden_4, keep_prob)\n",
    "        \n",
    "        return tf.matmul(hidden_drop_4, tf_layer5_pesi) + tf_layer5_bias\n",
    "    \n",
    "    # ESECUZIONE\n",
    "    logits = model(train_data)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                    labels=tf_train_target, logits=logits))  \n",
    "    # REGULARIZZATION\n",
    "    # N.B. La regolarizzazione si applica solitamente alle reti full-connected.\n",
    "    regularizers = tf.nn.l2_loss(tf_layer4_pesi) + tf.nn.l2_loss(tf_layer5_pesi)\n",
    "    loss = tf.reduce_mean(loss + 0.001 * regularizers)\n",
    "    \n",
    "    # DECAY\n",
    "    global_step = tf.Variable(0)  \n",
    "    start_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, \n",
    "                                               global_step, \n",
    "                                               100000, \n",
    "                                               0.96, \n",
    "                                               staircase=True)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # ADDESTRAMENTO\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    # VALIDAZIONE e TEST\n",
    "    valid_prediction = tf.nn.softmax(model(valid_data))\n",
    "    test_prediction = tf.nn.softmax(model(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
